{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________\n",
    "# About The Chard_Riesling Notebook\n",
    "_________\n",
    "\n",
    "\n",
    "* This notebook initially takes in a fully preprocessed csv with all observations and no accounting for imbalance. Another dataframe is then created that slices out observations where the value is either `CChardonnay` or `Riesling` in the `varietal` column.\n",
    "\n",
    "* Target and predictor variables are created from this dataframe, a baseline model is prepared and quickly analyzed, the df is split into training and testing sets, vectorized and then fit for modeling. \n",
    "\n",
    "* I fit three different models here based on the performance of the 20 class multiclassifier on this dataset. Random Forest, Logistic Regression and Multinomial Naive Bayes. \n",
    "\n",
    "* After this initial modeling I then run all three models after applying Synthetic Minority Oversampling Technique (SMOTE). \n",
    "\n",
    "**Summary of Results**\n",
    "\n",
    "SMOTE had a largely insiginificant effect on overall F1 Score. The only model that benefitted from SMOTE was Logistic Regression which saw no increase in its test score but saw it go from slightly overfit to neither under or overfit. \n",
    "\n",
    "____\n",
    "## Table of Contents\n",
    "_____\n",
    "\n",
    "<a id='Preprocessing'></a>\n",
    "* [Preprocessing](#Preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Imports\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sUcwDaHHHy0t",
    "outputId": "92e03474-399e-46fc-c8a2-1cacad29ea24"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Importing Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# SKLearn Libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# SKLearn Model Libraries\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# SKLearn Metric Libraries\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report\n",
    "from sklearn.metrics import roc_curve, plot_roc_curve, auc, f1_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# Other Libraries\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from random import sample\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QclIwFDdHy0v"
   },
   "source": [
    "_______\n",
    "# Read in Data\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QZmcTJNxHy0w"
   },
   "outputs": [],
   "source": [
    "# wine_df = pd.read_csv('/Users/jamesopacich/Documents/dsi/projects/capstone_archive/data/preprocessed_ready_4_model.csv') \n",
    "wine_df = pd.read_csv('../data/preprocessed_all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "Vav626KVHy0x",
    "outputId": "b44c7e56-6910-428c-af8a-509f06cc9402"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>varietal</th>\n",
       "      <th>description</th>\n",
       "      <th>color</th>\n",
       "      <th>parsed</th>\n",
       "      <th>parsed_w_stops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Riesling</td>\n",
       "      <td>pineapple rind lemon pith blossom start bit op...</td>\n",
       "      <td>white</td>\n",
       "      <td>pineapple rind lemon pith and orange blossom s...</td>\n",
       "      <td>pineapple rind lemon pith blossom start bit op...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   varietal                                        description  color  \\\n",
       "0  Riesling  pineapple rind lemon pith blossom start bit op...  white   \n",
       "\n",
       "                                              parsed  \\\n",
       "0  pineapple rind lemon pith and orange blossom s...   \n",
       "\n",
       "                                      parsed_w_stops  \n",
       "0  pineapple rind lemon pith blossom start bit op...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "varietal          0\n",
       "description       6\n",
       "color             0\n",
       "parsed            0\n",
       "parsed_w_stops    6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chardonnay    10779\n",
       "Riesling       4759\n",
       "Name: varietal, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chard_riesling = wine_df[(wine_df['varietal'] == 'Chardonnay') | (wine_df['varietal'] == 'Riesling')]\n",
    "chard_riesling['varietal'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Evwnhq0vHy1C"
   },
   "source": [
    "__________\n",
    "##  Create Target and Predictor Variables With Downsampled Data\n",
    "_________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "sK6_E6UyHy1L"
   },
   "outputs": [],
   "source": [
    "X = chard_riesling['parsed_w_stops']\n",
    "# Creating Multi-Class Targets\n",
    "y = chard_riesling['varietal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "n00fDTzaHy1M"
   },
   "outputs": [],
   "source": [
    "# Binarizing binary targets\n",
    "y = y.map({'Riesling': 0, 'Chardonnay': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQz2Tj6XHy1N",
    "outputId": "1259a92c-bedc-4d7e-afb7-ea7f862f8dfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Variables Shape: (15538,)\n",
      "Target Variables Shape: (15538,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Feature Variables Shape: {X.shape}')\n",
    "print(f'Target Variables Shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQyRxdo9Hy1O"
   },
   "source": [
    "_____\n",
    "# Baseline Model for Downsampled Data\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2NMn-epYHy1P",
    "outputId": "abf8955e-6a61-404d-85cd-59581a62f1f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.693719\n",
       "0    0.306281\n",
       "Name: varietal, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o996cf4JHy1R"
   },
   "source": [
    "____\n",
    "# Train / Test / Split\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "iKp4UQ1IHy1R"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size = .33, \n",
    "                                                   stratify = y, \n",
    "                                                   random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2Zmqf9wHy1R"
   },
   "source": [
    "_____\n",
    "# Grid Searching Hyperparameters\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "ttVlMw7XHy1R"
   },
   "outputs": [],
   "source": [
    "## creating a list of our two chosen vectorizers to iterate through in our grid search\n",
    "vectorizer = [CountVectorizer(), TfidfVectorizer()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "id": "pQH1yqbrHy1S"
   },
   "outputs": [],
   "source": [
    "## creating variables to accept tuning parameters\n",
    "max_feat = [300, 500]  \n",
    "ngram_range = [(1, 3), (1, 2)] \n",
    "stop_words = [None, 'english'] \n",
    "max_df = [0.9, 0.8] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "id": "0A9_uGLCHy1S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 192 candidates, totalling 576 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 232 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done 552 tasks      | elapsed:   27.2s\n",
      "[Parallel(n_jobs=-1)]: Done 576 out of 576 | elapsed:   31.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for CountVectorizer is: \n",
      "    0.8041\n",
      "    \n",
      "{'clf': RandomForestClassifier(min_samples_split=5, n_estimators=50), 'vect__max_df': 0.9, 'vect__max_features': 500, 'vect__ngram_range': (1, 2), 'vect__stop_words': None}\n",
      "\n",
      "Fitting 3 folds for each of 192 candidates, totalling 576 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 232 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done 552 tasks      | elapsed:   30.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for Tf-IDF Vectorizer is: \n",
      "    0.7929\n",
      "    \n",
      "{'clf': MultinomialNB(), 'clf__alpha': 1.0, 'vect__max_df': 0.9, 'vect__max_features': 500, 'vect__ngram_range': (1, 2), 'vect__stop_words': None}\n",
      "\n",
      "Below are the scores for each classifier. The DataFrame is named 'results_df' if you would like to access the information directly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 576 out of 576 | elapsed:   35.3s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf</th>\n",
       "      <th>param_clf__C</th>\n",
       "      <th>param_clf__penalty</th>\n",
       "      <th>param_vect__max_df</th>\n",
       "      <th>param_vect__max_features</th>\n",
       "      <th>param_vect__ngram_range</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "      <th>score_diff</th>\n",
       "      <th>high_plus_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.059980</td>\n",
       "      <td>0.005319</td>\n",
       "      <td>0.016179</td>\n",
       "      <td>0.004694</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.792899</td>\n",
       "      <td>0.012264</td>\n",
       "      <td>1</td>\n",
       "      <td>0.918768</td>\n",
       "      <td>0.932773</td>\n",
       "      <td>0.924581</td>\n",
       "      <td>0.925374</td>\n",
       "      <td>0.005745</td>\n",
       "      <td>-0.132475</td>\n",
       "      <td>0.660423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.069712</td>\n",
       "      <td>0.017944</td>\n",
       "      <td>0.014764</td>\n",
       "      <td>0.002302</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.792899</td>\n",
       "      <td>0.012264</td>\n",
       "      <td>1</td>\n",
       "      <td>0.918768</td>\n",
       "      <td>0.932773</td>\n",
       "      <td>0.924581</td>\n",
       "      <td>0.925374</td>\n",
       "      <td>0.005745</td>\n",
       "      <td>-0.132475</td>\n",
       "      <td>0.660423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.062699</td>\n",
       "      <td>0.006035</td>\n",
       "      <td>0.021562</td>\n",
       "      <td>0.008672</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.792899</td>\n",
       "      <td>0.012264</td>\n",
       "      <td>1</td>\n",
       "      <td>0.918768</td>\n",
       "      <td>0.932773</td>\n",
       "      <td>0.924581</td>\n",
       "      <td>0.925374</td>\n",
       "      <td>0.005745</td>\n",
       "      <td>-0.132475</td>\n",
       "      <td>0.660423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.056817</td>\n",
       "      <td>0.005196</td>\n",
       "      <td>0.022335</td>\n",
       "      <td>0.011261</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.792899</td>\n",
       "      <td>0.012264</td>\n",
       "      <td>1</td>\n",
       "      <td>0.918768</td>\n",
       "      <td>0.932773</td>\n",
       "      <td>0.924581</td>\n",
       "      <td>0.925374</td>\n",
       "      <td>0.005745</td>\n",
       "      <td>-0.132475</td>\n",
       "      <td>0.660423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.386081</td>\n",
       "      <td>0.035670</td>\n",
       "      <td>0.045306</td>\n",
       "      <td>0.009962</td>\n",
       "      <td>RandomForestClassifier(min_samples_split=5, n_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.791131</td>\n",
       "      <td>0.038431</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997199</td>\n",
       "      <td>0.003961</td>\n",
       "      <td>-0.206068</td>\n",
       "      <td>0.585062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.059905</td>\n",
       "      <td>0.005871</td>\n",
       "      <td>0.014256</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>LogisticRegression(solver='liblinear')</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.641778</td>\n",
       "      <td>0.012417</td>\n",
       "      <td>185</td>\n",
       "      <td>0.658263</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.639665</td>\n",
       "      <td>0.641793</td>\n",
       "      <td>0.012669</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.641763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.086117</td>\n",
       "      <td>0.004314</td>\n",
       "      <td>0.020793</td>\n",
       "      <td>0.003668</td>\n",
       "      <td>LogisticRegression(solver='liblinear')</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.638054</td>\n",
       "      <td>0.016127</td>\n",
       "      <td>189</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.639665</td>\n",
       "      <td>0.638058</td>\n",
       "      <td>0.008085</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.638049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.081431</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.015871</td>\n",
       "      <td>0.000557</td>\n",
       "      <td>LogisticRegression(solver='liblinear')</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.638054</td>\n",
       "      <td>0.016127</td>\n",
       "      <td>189</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.639665</td>\n",
       "      <td>0.638058</td>\n",
       "      <td>0.008085</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.638049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.092060</td>\n",
       "      <td>0.010118</td>\n",
       "      <td>0.015322</td>\n",
       "      <td>0.002541</td>\n",
       "      <td>LogisticRegression(solver='liblinear')</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.638054</td>\n",
       "      <td>0.016127</td>\n",
       "      <td>189</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.639665</td>\n",
       "      <td>0.638058</td>\n",
       "      <td>0.008085</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.638049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.076207</td>\n",
       "      <td>0.005082</td>\n",
       "      <td>0.014168</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>LogisticRegression(solver='liblinear')</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.638054</td>\n",
       "      <td>0.016127</td>\n",
       "      <td>189</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.639665</td>\n",
       "      <td>0.638058</td>\n",
       "      <td>0.008085</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.638049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "86        0.059980      0.005319         0.016179        0.004694   \n",
       "87        0.069712      0.017944         0.014764        0.002302   \n",
       "95        0.062699      0.006035         0.021562        0.008672   \n",
       "94        0.056817      0.005196         0.022335        0.011261   \n",
       "168       0.386081      0.035670         0.045306        0.009962   \n",
       "..             ...           ...              ...             ...   \n",
       "15        0.059905      0.005871         0.014256        0.001481   \n",
       "12        0.086117      0.004314         0.020793        0.003668   \n",
       "13        0.081431      0.002632         0.015871        0.000557   \n",
       "5         0.092060      0.010118         0.015322        0.002541   \n",
       "4         0.076207      0.005082         0.014168        0.000388   \n",
       "\n",
       "                                             param_clf param_clf__C  \\\n",
       "86                                     MultinomialNB()          NaN   \n",
       "87                                     MultinomialNB()          NaN   \n",
       "95                                     MultinomialNB()          NaN   \n",
       "94                                     MultinomialNB()          NaN   \n",
       "168  RandomForestClassifier(min_samples_split=5, n_...          NaN   \n",
       "..                                                 ...          ...   \n",
       "15              LogisticRegression(solver='liblinear')          0.5   \n",
       "12              LogisticRegression(solver='liblinear')          0.5   \n",
       "13              LogisticRegression(solver='liblinear')          0.5   \n",
       "5               LogisticRegression(solver='liblinear')          0.5   \n",
       "4               LogisticRegression(solver='liblinear')          0.5   \n",
       "\n",
       "    param_clf__penalty param_vect__max_df param_vect__max_features  \\\n",
       "86                 NaN                0.9                      500   \n",
       "87                 NaN                0.9                      500   \n",
       "95                 NaN                0.8                      500   \n",
       "94                 NaN                0.8                      500   \n",
       "168                NaN                0.8                      300   \n",
       "..                 ...                ...                      ...   \n",
       "15                  l1                0.8                      500   \n",
       "12                  l1                0.8                      500   \n",
       "13                  l1                0.8                      500   \n",
       "5                   l1                0.9                      500   \n",
       "4                   l1                0.9                      500   \n",
       "\n",
       "    param_vect__ngram_range  ... mean_test_score std_test_score  \\\n",
       "86                   (1, 2)  ...        0.792899       0.012264   \n",
       "87                   (1, 2)  ...        0.792899       0.012264   \n",
       "95                   (1, 2)  ...        0.792899       0.012264   \n",
       "94                   (1, 2)  ...        0.792899       0.012264   \n",
       "168                  (1, 3)  ...        0.791131       0.038431   \n",
       "..                      ...  ...             ...            ...   \n",
       "15                   (1, 2)  ...        0.641778       0.012417   \n",
       "12                   (1, 3)  ...        0.638054       0.016127   \n",
       "13                   (1, 3)  ...        0.638054       0.016127   \n",
       "5                    (1, 3)  ...        0.638054       0.016127   \n",
       "4                    (1, 3)  ...        0.638054       0.016127   \n",
       "\n",
       "    rank_test_score split0_train_score split1_train_score  split2_train_score  \\\n",
       "86                1           0.918768           0.932773            0.924581   \n",
       "87                1           0.918768           0.932773            0.924581   \n",
       "95                1           0.918768           0.932773            0.924581   \n",
       "94                1           0.918768           0.932773            0.924581   \n",
       "168               5           1.000000           0.991597            1.000000   \n",
       "..              ...                ...                ...                 ...   \n",
       "15              185           0.658263           0.627451            0.639665   \n",
       "12              189           0.647059           0.627451            0.639665   \n",
       "13              189           0.647059           0.627451            0.639665   \n",
       "5               189           0.647059           0.627451            0.639665   \n",
       "4               189           0.647059           0.627451            0.639665   \n",
       "\n",
       "     mean_train_score  std_train_score  score_diff  high_plus_diff  \n",
       "86           0.925374         0.005745   -0.132475        0.660423  \n",
       "87           0.925374         0.005745   -0.132475        0.660423  \n",
       "95           0.925374         0.005745   -0.132475        0.660423  \n",
       "94           0.925374         0.005745   -0.132475        0.660423  \n",
       "168          0.997199         0.003961   -0.206068        0.585062  \n",
       "..                ...              ...         ...             ...  \n",
       "15           0.641793         0.012669   -0.000015        0.641763  \n",
       "12           0.638058         0.008085   -0.000005        0.638049  \n",
       "13           0.638058         0.008085   -0.000005        0.638049  \n",
       "5            0.638058         0.008085   -0.000005        0.638049  \n",
       "4            0.638058         0.008085   -0.000005        0.638049  \n",
       "\n",
       "[192 rows x 28 columns]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## creating any empty results list to capture our cv_results_ at the end of each iteration\n",
    "results = []\n",
    "\n",
    "## looping through both vectorizers\n",
    "for vect in vectorizer:\n",
    "    \n",
    "    #### Pipeline for our Vectorizer and Classifier Models ####\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', vect),\n",
    "        ('clf', LogisticRegression())])\n",
    "    \n",
    "    instantiations = [ #### Beginning of Instantiations List ####\n",
    "        {\n",
    "            ### Log Reg Vect Hyperparameters ###\n",
    "            \n",
    "            'vect__max_features': max_feat,\n",
    "            'vect__stop_words': stop_words,\n",
    "            'vect__ngram_range': ngram_range,\n",
    "            'vect__max_df': max_df,\n",
    "            \n",
    "            ## Instantiate Log Reg and Hyperparamaters\n",
    "            'clf': (LogisticRegression(solver='liblinear'), ), ## setting our first classifier model\n",
    "            'clf__penalty': ('l1', 'l2'),\n",
    "            'clf__C': (.5, 1.0), \n",
    "        }, \n",
    "    #######\n",
    "        {\n",
    "            \n",
    "            ### Multinomial Bayes Vect Hyperparameters ###\n",
    "            'vect__max_features': max_feat,\n",
    "            'vect__stop_words': stop_words,\n",
    "            'vect__ngram_range': ngram_range,\n",
    "            'vect__max_df': max_df,\n",
    "            \n",
    "            ## Instantiate Mulinomial NB and Hyperparamters ##\n",
    "            'clf': (MultinomialNB(), ),  \n",
    "            'clf__alpha': (.5, 1.0)\n",
    "        },\n",
    "    #######   \n",
    "        {\n",
    "            ### SVC Vect Hyperparameters ###\n",
    "            'vect__max_features': max_feat,\n",
    "            'vect__stop_words': stop_words,\n",
    "            'vect__ngram_range': ngram_range,\n",
    "            'vect__max_df': max_df,\n",
    "            \n",
    "            ## KNN Instantiation and Hyperparameters\n",
    "            'clf': (KNeighborsClassifier(), ),\n",
    "            'clf__n_neighbors': (5 , 10),\n",
    "            'clf__weights': ('uniform', 'distance')\n",
    "        },\n",
    "    #######  \n",
    "        {\n",
    "            ### RandomForestClassifier Vect Hyperparameters ###\n",
    "            'vect__max_features': max_feat,\n",
    "            'vect__stop_words': stop_words,\n",
    "            'vect__ngram_range': ngram_range,\n",
    "            'vect__max_df': max_df,\n",
    "            \n",
    "            ## Instantiate RandomForestClassifier ##\n",
    "            'clf': (RandomForestClassifier(n_estimators=50, min_samples_split=5), ),\n",
    "        },\n",
    "    #######\n",
    "        {\n",
    "            ### Voting Classifier Vect Hyperparameters ###\n",
    "            'vect__max_features': max_feat,\n",
    "            'vect__stop_words': stop_words,\n",
    "            'vect__ngram_range': ngram_range,\n",
    "            'vect__max_df': max_df,\n",
    "            \n",
    "            ## Instantiating Ensemble Voting Classifier ##\n",
    "            'clf': (VotingClassifier(estimators=[('lr', LogisticRegression()), \n",
    "                                                 ('rf', RandomForestClassifier()), \n",
    "                                                 ('mnb', MultinomialNB()), ],                                           \n",
    "                                            voting='hard'), )\n",
    "        }    \n",
    "                    ] #### end of instantiations list ####\n",
    "    \n",
    "    \n",
    "    #### Grid Search ####\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, \n",
    "                               instantiations,\n",
    "                               cv=3,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=3,\n",
    "                               return_train_score=True)\n",
    "    \n",
    "    #### Output Results ####\n",
    "    \n",
    "    ## running an if statement to print the type of vectorizer used\n",
    "    if vect == vectorizer[0]:\n",
    "        vect_string = \"CountVectorizer\"\n",
    "    \n",
    "    else:\n",
    "        vect_string = \"Tf-IDF Vectorizer\"\n",
    "    \n",
    "    ## fitting our model and printing our best scores and parameters\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f'''Best score for {vect_string} is: \n",
    "    {round(grid_search.best_score_, 4)}\n",
    "    ''')\n",
    "    print(grid_search.best_params_)\n",
    "    print(\"\")\n",
    "    \n",
    "    ## appending our cv_results_ to the end of results\n",
    "    results.append(grid_search.cv_results_)\n",
    "    \n",
    "# Create a Dataframe of the Results\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "results_df['score_diff'] = results_df['mean_test_score'] - results_df['mean_train_score']\n",
    "results_df = results_df.sort_values('mean_test_score', ascending = False)\n",
    "\n",
    "print(f\"Below are the scores for each classifier. The DataFrame is named 'results_df' if you would like to access the information directly.\")\n",
    "\n",
    "# Print out Df with results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_group = results_df[['param_clf', 'rank_test_score', 'mean_test_score',  'mean_train_score', 'std_test_score', 'std_train_score', 'score_diff', 'high_plus_diff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-231-542611792e4b>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results_df_group['param_clf'] = results_df_group['param_clf'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "results_df_group['param_clf'] = results_df_group['param_clf'].astype(str)\n",
    "results_df_group = results_df_group.groupby('param_clf').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "      <th>score_diff</th>\n",
       "      <th>high_plus_diff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier()</th>\n",
       "      <td>120.5000</td>\n",
       "      <td>0.746599</td>\n",
       "      <td>0.916739</td>\n",
       "      <td>0.019540</td>\n",
       "      <td>0.006318</td>\n",
       "      <td>-0.170141</td>\n",
       "      <td>0.576458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression(solver='liblinear')</th>\n",
       "      <td>109.0625</td>\n",
       "      <td>0.733441</td>\n",
       "      <td>0.814715</td>\n",
       "      <td>0.011814</td>\n",
       "      <td>0.012165</td>\n",
       "      <td>-0.081274</td>\n",
       "      <td>0.652167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB()</th>\n",
       "      <td>43.2500</td>\n",
       "      <td>0.783588</td>\n",
       "      <td>0.917904</td>\n",
       "      <td>0.009509</td>\n",
       "      <td>0.008260</td>\n",
       "      <td>-0.134316</td>\n",
       "      <td>0.649271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier(min_samples_split=5, n_estimators=50)</th>\n",
       "      <td>87.5625</td>\n",
       "      <td>0.770087</td>\n",
       "      <td>0.997377</td>\n",
       "      <td>0.016669</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>-0.227291</td>\n",
       "      <td>0.542796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VotingClassifier(estimators=[('lr', LogisticRegression()),\\n                             ('rf', RandomForestClassifier()),\\n                             ('mnb', MultinomialNB())])</th>\n",
       "      <td>43.1875</td>\n",
       "      <td>0.783017</td>\n",
       "      <td>0.934699</td>\n",
       "      <td>0.010519</td>\n",
       "      <td>0.005005</td>\n",
       "      <td>-0.151682</td>\n",
       "      <td>0.631336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    rank_test_score  \\\n",
       "param_clf                                                             \n",
       "KNeighborsClassifier()                                     120.5000   \n",
       "LogisticRegression(solver='liblinear')                     109.0625   \n",
       "MultinomialNB()                                             43.2500   \n",
       "RandomForestClassifier(min_samples_split=5, n_e...          87.5625   \n",
       "VotingClassifier(estimators=[('lr', LogisticReg...          43.1875   \n",
       "\n",
       "                                                    mean_test_score  \\\n",
       "param_clf                                                             \n",
       "KNeighborsClassifier()                                     0.746599   \n",
       "LogisticRegression(solver='liblinear')                     0.733441   \n",
       "MultinomialNB()                                            0.783588   \n",
       "RandomForestClassifier(min_samples_split=5, n_e...         0.770087   \n",
       "VotingClassifier(estimators=[('lr', LogisticReg...         0.783017   \n",
       "\n",
       "                                                    mean_train_score  \\\n",
       "param_clf                                                              \n",
       "KNeighborsClassifier()                                      0.916739   \n",
       "LogisticRegression(solver='liblinear')                      0.814715   \n",
       "MultinomialNB()                                             0.917904   \n",
       "RandomForestClassifier(min_samples_split=5, n_e...          0.997377   \n",
       "VotingClassifier(estimators=[('lr', LogisticReg...          0.934699   \n",
       "\n",
       "                                                    std_test_score  \\\n",
       "param_clf                                                            \n",
       "KNeighborsClassifier()                                    0.019540   \n",
       "LogisticRegression(solver='liblinear')                    0.011814   \n",
       "MultinomialNB()                                           0.009509   \n",
       "RandomForestClassifier(min_samples_split=5, n_e...        0.016669   \n",
       "VotingClassifier(estimators=[('lr', LogisticReg...        0.010519   \n",
       "\n",
       "                                                    std_train_score  \\\n",
       "param_clf                                                             \n",
       "KNeighborsClassifier()                                     0.006318   \n",
       "LogisticRegression(solver='liblinear')                     0.012165   \n",
       "MultinomialNB()                                            0.008260   \n",
       "RandomForestClassifier(min_samples_split=5, n_e...         0.001925   \n",
       "VotingClassifier(estimators=[('lr', LogisticReg...         0.005005   \n",
       "\n",
       "                                                    score_diff  high_plus_diff  \n",
       "param_clf                                                                       \n",
       "KNeighborsClassifier()                               -0.170141        0.576458  \n",
       "LogisticRegression(solver='liblinear')               -0.081274        0.652167  \n",
       "MultinomialNB()                                      -0.134316        0.649271  \n",
       "RandomForestClassifier(min_samples_split=5, n_e...   -0.227291        0.542796  \n",
       "VotingClassifier(estimators=[('lr', LogisticReg...   -0.151682        0.631336  "
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "# Chard/Riesling Model with Best Performing Algos\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WZUs4ZkHy1f"
   },
   "source": [
    "_____\n",
    "# Vectorizing\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoMdZOjOHy1f"
   },
   "source": [
    "{'clf': RandomForestClassifier(min_samples_split=5, n_estimators=50), 'vect__max_df': 0.8, 'vect__max_features': 500, 'vect__ngram_range': (1, 3), 'vect__stop_words': None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wvaTlcIBHy1f",
    "outputId": "ee545554-6cd6-481a-e7b4-7a0693909014"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.8, max_features=500, ngram_range=(1, 3))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate CountVectorizer()\n",
    "tvec = TfidfVectorizer(max_df = .8, max_features = 500, \n",
    "                       ngram_range = (1,3))\n",
    "\n",
    "# fit CountVectorizer()\n",
    "tvec.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "XGmewI2BHy1g"
   },
   "outputs": [],
   "source": [
    "# Transform the corpus on training data\n",
    "X_train = tvec.transform(X_train)\n",
    "X_test = tvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59g76X_vHy1e"
   },
   "source": [
    "_____\n",
    "# Random Forest\n",
    "______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "tXVnZr18Hy1j"
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(min_samples_split = 5, n_estimators = 110, )\n",
    "forest.fit(X_train, y_train)\n",
    "train_preds = forest.predict(X_train)\n",
    "preds = forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2FYCBZzpHy1j",
    "outputId": "a6917ee5-4d88-4bef-c9f3-a39f5dd87c8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1 Score: 99.51\n",
      "Test F1 Score: 90.14\n"
     ]
    }
   ],
   "source": [
    "# Testing Score\n",
    "score = metrics.f1_score(y_test, preds, average = 'binary')\n",
    "\n",
    "# Training Score\n",
    "score_train = metrics.f1_score(y_train, train_preds, average = 'binary')\n",
    "\n",
    "print(f'Training F1 Score: {round(score_train * 100, 2)}')\n",
    "print(f'Test F1 Score: {round(score * 100, 2)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "# Multinomial Naive Bayes\n",
    "_____\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(alpha = 1.0)\n",
    "mnb.fit(X_train, y_train)\n",
    "train_preds = mnb.predict(X_train)\n",
    "preds = mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1 Score: 90.78\n",
      "Test F1 Score: 90.46\n"
     ]
    }
   ],
   "source": [
    "# Testing Score\n",
    "score = metrics.f1_score(y_test, preds, average = 'binary')\n",
    "\n",
    "# Training Score\n",
    "score_train = metrics.f1_score(y_train, train_preds, average = 'binary')\n",
    "\n",
    "print(f'Training F1 Score: {round(score_train * 100, 2)}')\n",
    "print(f'Test F1 Score: {round(score * 100, 2)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJ0eaVNLHy1k"
   },
   "source": [
    "_____\n",
    "# Logisitic Regression\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf': LogisticRegression(penalty='l1', solver='liblinear'),\n",
       " 'clf__C': 1.0,\n",
       " 'clf__penalty': 'l1',\n",
       " 'vect__max_df': 0.9,\n",
       " 'vect__max_features': 500,\n",
       " 'vect__ngram_range': (1, 2),\n",
       " 'vect__stop_words': 'english'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'clf': LogisticRegression(penalty='l1', solver='liblinear'), 'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_df': 0.9, 'vect__max_features': 500, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "I_GZq3-LHy1k"
   },
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(penalty = 'l1', solver = 'liblinear')\n",
    "log_reg.fit(X_train, y_train)\n",
    "train_preds = log_reg.predict(X_train)\n",
    "preds = mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "qf7JVL41Hy1k",
    "outputId": "5f25a9b2-3c69-430d-bb6c-09063386e99f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1 Score: 92.15\n",
      "Test F1 Score: 90.46\n"
     ]
    }
   ],
   "source": [
    "# Testing Score\n",
    "\n",
    "\n",
    "\n",
    "score = metrics.f1_score(y_test, preds, average = 'binary')\n",
    "\n",
    "# Training Score\n",
    "score_train = metrics.f1_score(y_train, train_preds, average = 'binary')\n",
    "\n",
    "print(f'Training F1 Score: {round(score_train * 100, 2)}')\n",
    "print(f'Test F1 Score: {round(score * 100, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNGEDYFWHy1k"
   },
   "source": [
    "# Bootstrapped Models\n",
    "\n",
    "#### SMOTE \n",
    "\n",
    "\"SMOTE stands for **Synthetic Minority Oversampling Technique.** This is a statistical technique for **increasing the number of cases in your dataset in a balanced way.** The module works by generating new instances from existing minority cases that you supply as input. This implementation of SMOTE does not change the number of majority cases.\n",
    "\n",
    "The **new instances are not just copies** of existing minority cases; instead, the algorithm takes samples of the feature space for each target class and its nearest neighbors, and generates new examples that combine features of the target case with features of its neighbors. This approach increases the features available to each class and makes the samples more general.\n",
    "\n",
    "SMOTE takes the entire dataset as an input, but it increases the percentage of only the minority cases. For example, suppose you have an imbalanced dataset where just 1% of the cases have the target value A (the minority class), and 99% of the cases have the value B. To increase the percentage of minority cases to twice the previous percentage, you would enter 200 for SMOTE percentage in the module's properties.\"\n",
    "\n",
    "* https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/smote#:~:text=SMOTE%20stands%20for%20Synthetic%20Minority,dataset%20in%20a%20balanced%20way.&text=SMOTE%20takes%20the%20entire%20dataset,of%20only%20the%20minority%20cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "qbOoa83sHy1l"
   },
   "outputs": [],
   "source": [
    "def smote_oversample(x, y):\n",
    "\n",
    "    # Instantiate Oversampler\n",
    "    oversample = SMOTE()\n",
    "    \n",
    "    # fit oversampler\n",
    "    x, y = oversample.fit_resample(x, y)\n",
    "    \n",
    "    # Counter creates a dict that summarizes the distribution\n",
    "    counter = Counter(y)\n",
    "\n",
    "    for key, value in counter.items():\n",
    "        \n",
    "        # create variable with percentage of samples of each variable \n",
    "        percent = round(value / len(y_train) * 100, 2)\n",
    "        \n",
    "    # create df of target labels\n",
    "    ovr_sampled = pd.DataFrame(counter.keys(), columns = ['target_label'])\n",
    "    # append number of samples to df\n",
    "    ovr_sampled['num_samples'] = counter.values()\n",
    "    # append percentage of samples to df\n",
    "    ovr_sampled['percent'] = percent\n",
    "\n",
    "    # Create a plot showing the balanced classes\n",
    "    plt.bar(ovr_sampled['target_label'], ovr_sampled['percent'])\n",
    "    \n",
    "    plt.title('Class Labels as Percentage of Samples')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Percentage of Labels')\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.show(); \n",
    "    \n",
    "    # return df of sample distributions\n",
    "    print(ovr_sampled)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 815
    },
    "id": "5HkfUVW3Hy1l",
    "outputId": "5f9c5751-301c-49e2-afcc-0c20741ea1e7",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target_label  num_samples  percent\n",
      "0             1         7222    69.38\n",
      "1             0         7222    69.38\n"
     ]
    }
   ],
   "source": [
    "XS_train, ys_train = smote_oversample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "wjRUd6HlHy1l"
   },
   "outputs": [],
   "source": [
    "smote_mnb = MultinomialNB(alpha = 1.0)\n",
    "smote_mnb.fit(XS_train, ys_train)\n",
    "train_preds = smote_mnb.predict(XS_train)\n",
    "preds = smote_mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jqlqfRL8Hy1l",
    "outputId": "23fa739d-70e4-453c-9813-ab5a2daaca29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1 Score: 85.45\n",
      "Test F1 Score: 88.46\n"
     ]
    }
   ],
   "source": [
    "# Testing Score\n",
    "score = metrics.f1_score(y_test, preds, average = 'binary')\n",
    "\n",
    "# Training Score\n",
    "score_train = metrics.f1_score(ys_train, train_preds, average = 'binary')\n",
    "\n",
    "print(f'Training F1 Score: {round(score_train * 100, 2)}')\n",
    "print(f'Test F1 Score: {round(score * 100, 2)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "8h83XzSBHy1m"
   },
   "outputs": [],
   "source": [
    "smote_log_reg = LogisticRegression(solver = 'liblinear')\n",
    "smote_log_reg.fit(XS_train, ys_train)\n",
    "train_preds = smote_log_reg.predict(XS_train)\n",
    "preds = smote_log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KdQT6VjOHy1m",
    "outputId": "1e94373b-07b8-4ad5-d515-f21536c9ece3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1 Score: 88.44\n",
      "Test F1 Score: 88.98\n"
     ]
    }
   ],
   "source": [
    "# Testing Score\n",
    "score = metrics.f1_score(y_test, preds, average = 'binary')\n",
    "\n",
    "# Training Score\n",
    "score_train = metrics.f1_score(ys_train, train_preds, average = 'binary')\n",
    "\n",
    "print(f'Training F1 Score: {round(score_train * 100, 2)}')\n",
    "print(f'Test F1 Score: {round(score * 100, 2)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of 003_downsample_grid.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
